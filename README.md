# Azure AI Foundry Chat Application

> **Disclaimer:** All files in this repository are generated by GitHub Copilot.

This project provides a Streamlit-based chat interface integrated with Azure OpenAI services. It allows users to interact with various Azure AI models, upload context files (text, PDF, images), and receive AI-generated responses.

## Features

- Interactive chat interface powered by Streamlit.
- Integration with Azure OpenAI models (GPT-3.5 Turbo, GPT-4o, Deepseek-R1, O3 Mini).
- Supports uploading context files (text, PDF, images) for enhanced interactions.

## Prerequisites

- Docker installed locally ([Docker Desktop](https://www.docker.com/products/docker-desktop/)).
- Azure subscription with access to Azure OpenAI services.
- Azure CLI installed ([Azure CLI](https://docs.microsoft.com/cli/azure/install-azure-cli)).

## Environment Variables

Ensure you have the following environment variables set:

- `AZURE_OPENAI_ENDPOINT`: Your Azure OpenAI endpoint URL.
- `AZURE_OPENAI_API_KEY`: Your Azure OpenAI API key.

## Building and Running Locally with Docker

1. **Build the Docker image**:

```bash
docker build -t azure-ai-chat-app .
```

2. **Run the Docker container locally**:

```bash
docker run -p 8501:8501 -e AZURE_OPENAI_ENDPOINT="<your-endpoint>" -e AZURE_OPENAI_API_KEY="<your-api-key>" azure-ai-chat-app
```

3. **Access the application**:

Open your browser and navigate to:

```
http://localhost:8501
```

## Deploying to Azure Container Instances (ACI)

1. **Log in to Azure CLI**:

```bash
az login
```

2. **Create a resource group (if not already created)**:

```bash
az group create --name <your-resource-group> --location <your-location>
```

3. **Deploy using Bicep template** (`infra/aci.bicep`):

```bash
az deployment group create \
  --resource-group <your-resource-group> \
  --template-file infra/aci.bicep \
  --parameters image="<your-docker-image>" azureOpenAIEndpoint="<your-endpoint>" azureOpenAIAPIKey="<your-api-key>"
```

Replace placeholders (`<your-resource-group>`, `<your-location>`, `<your-docker-image>`, `<your-endpoint>`, `<your-api-key>`) with your actual values.

4. **Access the deployed application**:

After deployment, the fully qualified domain name (FQDN) will be displayed. Access your application at:

```
http://<your-container-group-name>.<region>.azurecontainer.io:8501
```

## Original Prompts
> Create a Python web app using Streamlit that allows me to call an Azure AI Foundry Endpoint to chat with. I want the user to be able to select a model (listed in a parameter in the app to make sure the developer has a choice which models are allowed). Provide the ability to upload a text file, PDF or image for context in the chat. The endpoint and API key should be configurable using an environment variable but also provide an input option in the web app itself so it can be changed at runtime.

> Create a README file for Github using Markup. Please add documentation about the project, also add instructions on how to build the docker file and run it locally next to the instructions on how to run this to Azure Container Instances. Feel free to add this prompt to the readme too.

> Create a bicep and a terraform file to deploy the Azure Container Intances.